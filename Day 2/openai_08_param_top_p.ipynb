{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9b7901-a6c6-42b2-898c-7d6e3c18eab6",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### **Top-p Sampling in OpenAI GPT**\n",
    "----------------------------------\n",
    "\n",
    "**Description:**\n",
    "- Top-p sampling, also known as \"nucleus sampling,\" is a decoding technique used during text generation.\n",
    "- Instead of considering all possible tokens during generation (as in greedy decoding), top-p sampling focuses on a subset of tokens whose cumulative probability is above a specified threshold \\( p \\).\n",
    "- This allows the model to dynamically adjust the number of tokens it considers at each step based on the probability distribution of the tokens.\n",
    "\n",
    "- **top_p = 1**: This includes all tokens in the probability distribution (equivalent to no restriction).\n",
    "- **top_p < 1**: The model samples from a smaller set of high-probability tokens, adding randomness while still maintaining some control over the coherence of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b724c-161c-4585-9027-9ad6301419d2",
   "metadata": {},
   "source": [
    "**Example Illustration:**\n",
    "- If `top_p = 0.9`, the model will consider the smallest subset of tokens whose cumulative probability is at least 90%. The remaining tokens are ignored, preventing low-probability (often irrelevant) tokens from being selected.\n",
    "  \n",
    "For instance, if a model predicts the following probabilities for the next word:\n",
    "- \"dog\": 0.4\n",
    "- \"cat\": 0.3\n",
    "- \"mouse\": 0.2\n",
    "- \"elephant\": 0.05\n",
    "- \"lion\": 0.05\n",
    "\n",
    "With **top_p = 0.9**, only the first three words (\"dog\", \"cat\", \"mouse\") will be considered, as their cumulative probability is 0.9. The tokens \"elephant\" and \"lion\" will be excluded from sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c84ce-6ba4-4991-b2b3-1211d5ff5ae4",
   "metadata": {},
   "source": [
    "#### Greedy Decoding:\n",
    "- **How it works:** Selects the token with the highest probability at each step.\n",
    "- **Result:** Deterministic and often repetitive output.\n",
    "- **Use case:** Best for short, factual responses where randomness is not desired.\n",
    "\n",
    "#### Top-p Sampling:\n",
    "- **How it works:** Selects a token from the smallest subset of tokens whose cumulative probability meets or exceeds the threshold `p`. Randomly samples from this subset.\n",
    "- **Result:** More creative and varied output, with controlled randomness.\n",
    "- **Use case:** Ideal for tasks that benefit from creativity or diverse responses, like story generation or dialogue systems.\n",
    "\n",
    "#### Key Difference:\n",
    "- Greedy decoding picks the single highest-probability token every time.\n",
    "- Top-p sampling considers a range of tokens and samples from them, depending on the set defined by `p`, leading to more diverse outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb52e33-72be-441d-b797-5a257f6e65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bef0ef0-87cb-41a5-9078-d9d70edcfd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7d4353-196b-4f48-bda8-2a9bf61a2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3247ec01-1ae6-4a95-b656-149295199a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Once upon a time, in a distant land, there was a kingdom ruled by a wise and just king who\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04ce2b9-3cc6-49b7-8b11-d565e4aa066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was loved by all his subjects. The kingdom prospered under his rule, and the people lived in peace and harmony.\n",
      "\n",
      "One day, a dark cloud appeared on the horizon as rumors spread of an impending invasion by a neighboring kingdom. The king knew that\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding with temperature 0\n",
    "response_greedy = client.chat.completions.create(\n",
    "        model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "        messages   = [\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        max_tokens = 50,\n",
    "        temperature= 0.0,  # No randomness, greedy decoding\n",
    "    )\n",
    "\n",
    "print(response_greedy.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a774b7b1-a1d5-487c-b003-a397f9c61769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was loved by all his subjects. The kingdom prospered under his rule, and the people lived in peace and harmony.\n",
      "\n",
      "One day, a dark cloud appeared on the horizon as rumors spread of an impending invasion by a neighboring kingdom. The king knew that he had to act quickly to protect his people and his kingdom.\n",
      "\n",
      "He called upon his most trusted advisors and together they devised a plan to defend the kingdom. The king rallied his army and led them into battle with courage and determination.\n",
      "\n",
      "After a fierce and bloody struggle, the kingdom emerged victorious, thanks to the leadership and bravery of their king. The people rejoiced and celebrated their victory, grateful for their wise and just ruler.\n",
      "\n",
      "From that day on, the king was hailed as a hero and his legacy\n"
     ]
    }
   ],
   "source": [
    "# top_p decoding with temperature 0\n",
    "response_greedy = client.chat.completions.create(\n",
    "        model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "        messages   = [\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        max_tokens = 150,\n",
    "        top_p      = 0.15,  # Top-p sampling for creative diversity\n",
    "        temperature= 0.0    # Introduce a bit of randomness\n",
    "    )\n",
    "\n",
    "print(response_greedy.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dfb68-7512-4937-b96b-9424391ce557",
   "metadata": {},
   "source": [
    "#### Key Differences in How `temperature` and `top_p` Influence Randomness\n",
    "\n",
    "##### Temperature alone can introduce randomness:\n",
    "- **What it does**: It scales the entire probability distribution. Lowering `temperature` makes the model more deterministic (similar to greedy decoding), while raising it allows for more exploration of less probable tokens.\n",
    "- **Effect**: It introduces **randomness globally** across all tokens.\n",
    "\n",
    "##### Top-p alone can influence randomness, but in a more controlled way:\n",
    "- **What it does**: It restricts the model to choosing from a subset of the most probable tokens whose cumulative probability is within the threshold `p`. It still randomly samples from this subset.\n",
    "- **Effect**: It introduces **randomness locally** within the top probable tokens but prevents the model from choosing extremely low-probability tokens.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why `top_p` Works Without Temperature:\n",
    "Even without setting `temperature`, **top-p sampling** (or nucleus sampling) introduces randomness by restricting the token pool, ensuring the model samples from the top `p%` of probable tokens. \n",
    "\n",
    "##### Example:\n",
    "- If you set `top_p = 0.9`, the model will sample tokens only from the subset where their cumulative probability is 90%. \n",
    "- This **limits randomness** to the most probable tokens but still samples randomly from this subset.\n",
    "- **Temperature is not required** for top-p to work because it inherently involves randomness in selecting among the probable tokens.\n",
    "\n",
    "---\n",
    "\n",
    "##### When Does `top_p` Need Temperature?\n",
    "\n",
    "- **top_p** restricts the token pool to the most probable tokens.\n",
    "- **temperature** controls the level of randomness in choosing from that pool.\n",
    "\n",
    "When you combine both:\n",
    "- `top_p` defines the **subset** of most probable tokens.\n",
    "- `temperature` **smooths the probabilities** within the restricted token pool, allowing a gradient of selection, rather than sharp deterministic selections.\n",
    "\n",
    "By setting both, you gain **finer control** over the randomness in the generated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee42a8-22eb-4430-89c0-c375c4833882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
