{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9672a43-7fc3-42e2-8364-149dd21f4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dot\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d56fe5-b5c5-4bd9-94a8-1fd427cee2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Dataset\n",
    "dataset = [\n",
    "    \"COVID-19 is a novel coronavirus\",\n",
    "    \"The virus spreads through droplets\",\n",
    "    \"Symptoms include fever and cough\",\n",
    "    \"Social distancing helps prevent spread\",\n",
    "    \"Masks are essential for protection\",\n",
    "    \"Vaccines have been developed\",\n",
    "    \"COVID-19 affects the respiratory system\",\n",
    "    \"Hand hygiene is crucial to prevent infection\",\n",
    "    \"Quarantine and isolation are necessary\",\n",
    "    \"The pandemic has impacted global health\",\n",
    "    \"Travel restrictions have been implemented\",\n",
    "    \"Testing and contact tracing are important\",\n",
    "    \"Remote work has become common\",\n",
    "    \"Lockdowns have been imposed in many areas\",\n",
    "    \"Public health measures are vital\",\n",
    "    \"The virus can survive on surfaces\",\n",
    "    \"Research is ongoing for treatments\",\n",
    "    \"The virus mutates, creating new strains\",\n",
    "    \"Many people have recovered from COVID-19\",\n",
    "    \"The pandemic has affected mental health\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "311f44e1-82f7-45e4-b67c-e6287cd09330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preparation\n",
    "# Tokenize the dataset\n",
    "tokenizer = Tokenizer(filters= '!\"#$%&()*+,./:;<=>?@[\\\\]^`{|}~\\t\\n', # '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower  = False)\n",
    "\n",
    "tokenizer.fit_on_texts(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04bcbcd8-d88d-4849-a4df-11a8922839e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('COVID-19', 3), ('is', 3), ('a', 1), ('novel', 1), ('coronavirus', 1), ('The', 5), ('virus', 3), ('spreads', 1), ('through', 1), ('droplets', 1), ('Symptoms', 1), ('include', 1), ('fever', 1), ('and', 3), ('cough', 1), ('Social', 1), ('distancing', 1), ('helps', 1), ('prevent', 2), ('spread', 1), ('Masks', 1), ('are', 4), ('essential', 1), ('for', 2), ('protection', 1), ('Vaccines', 1), ('have', 4), ('been', 3), ('developed', 1), ('affects', 1), ('the', 1), ('respiratory', 1), ('system', 1), ('Hand', 1), ('hygiene', 1), ('crucial', 1), ('to', 1), ('infection', 1), ('Quarantine', 1), ('isolation', 1), ('necessary', 1), ('pandemic', 2), ('has', 3), ('impacted', 1), ('global', 1), ('health', 3), ('Travel', 1), ('restrictions', 1), ('implemented', 1), ('Testing', 1), ('contact', 1), ('tracing', 1), ('important', 1), ('Remote', 1), ('work', 1), ('become', 1), ('common', 1), ('Lockdowns', 1), ('imposed', 1), ('in', 1), ('many', 1), ('areas', 1), ('Public', 1), ('measures', 1), ('vital', 1), ('can', 1), ('survive', 1), ('on', 1), ('surfaces', 1), ('Research', 1), ('ongoing', 1), ('treatments', 1), ('mutates', 1), ('creating', 1), ('new', 1), ('strains', 1), ('Many', 1), ('people', 1), ('recovered', 1), ('from', 1), ('affected', 1), ('mental', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "252e1858-9777-4fa0-a500-ab1656778962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'are': 2, 'have': 3, 'COVID-19': 4, 'is': 5, 'virus': 6, 'and': 7, 'been': 8, 'has': 9, 'health': 10, 'prevent': 11, 'for': 12, 'pandemic': 13, 'a': 14, 'novel': 15, 'coronavirus': 16, 'spreads': 17, 'through': 18, 'droplets': 19, 'Symptoms': 20, 'include': 21, 'fever': 22, 'cough': 23, 'Social': 24, 'distancing': 25, 'helps': 26, 'spread': 27, 'Masks': 28, 'essential': 29, 'protection': 30, 'Vaccines': 31, 'developed': 32, 'affects': 33, 'the': 34, 'respiratory': 35, 'system': 36, 'Hand': 37, 'hygiene': 38, 'crucial': 39, 'to': 40, 'infection': 41, 'Quarantine': 42, 'isolation': 43, 'necessary': 44, 'impacted': 45, 'global': 46, 'Travel': 47, 'restrictions': 48, 'implemented': 49, 'Testing': 50, 'contact': 51, 'tracing': 52, 'important': 53, 'Remote': 54, 'work': 55, 'become': 56, 'common': 57, 'Lockdowns': 58, 'imposed': 59, 'in': 60, 'many': 61, 'areas': 62, 'Public': 63, 'measures': 64, 'vital': 65, 'can': 66, 'survive': 67, 'on': 68, 'surfaces': 69, 'Research': 70, 'ongoing': 71, 'treatments': 72, 'mutates': 73, 'creating': 74, 'new': 75, 'strains': 76, 'Many': 77, 'people': 78, 'recovered': 79, 'from': 80, 'affected': 81, 'mental': 82}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dea54c6a-ea8c-4291-8d9b-5637488e7ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d04a7ae-42dd-4a03-88d7-f3e35cf2b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10879096-5b4f-4431-bd01-3954266db27f",
   "metadata": {},
   "source": [
    "Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "Only top `num_words-1` most frequent words will be taken into account.\n",
    "Only words known by the tokenizer will be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a393821-1c88-45b7-b6f3-44cc2f000b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 14, 15, 16],\n",
       " [1, 6, 17, 18, 19],\n",
       " [20, 21, 22, 7, 23],\n",
       " [24, 25, 26, 11, 27],\n",
       " [28, 2, 29, 12, 30],\n",
       " [31, 3, 8, 32],\n",
       " [4, 33, 34, 35, 36],\n",
       " [37, 38, 5, 39, 40, 11, 41],\n",
       " [42, 7, 43, 2, 44],\n",
       " [1, 13, 9, 45, 46, 10],\n",
       " [47, 48, 3, 8, 49],\n",
       " [50, 7, 51, 52, 2, 53],\n",
       " [54, 55, 9, 56, 57],\n",
       " [58, 3, 8, 59, 60, 61, 62],\n",
       " [63, 10, 64, 2, 65],\n",
       " [1, 6, 66, 67, 68, 69],\n",
       " [70, 5, 71, 12, 72],\n",
       " [1, 6, 73, 74, 75, 76],\n",
       " [77, 78, 3, 79, 80, 4],\n",
       " [1, 13, 9, 81, 82, 10]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate skip-grams\n",
    "sequences = tokenizer.texts_to_sequences(dataset)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc9026-09d8-4348-bcd9-15df0c7fbb61",
   "metadata": {},
   "source": [
    "#### Generate skipgram word pairs\n",
    "\n",
    "**`skipgrams` Function in Keras**\n",
    "\n",
    "The `skipgrams` function in Keras is used to generate training pairs for the Word2Vec model using the skip-gram approach. \n",
    "\n",
    "The skip-gram model predicts the surrounding context words given a target word. \n",
    "\n",
    "The `skipgrams` function generates pairs of `(target, context)` words from sequences of word indices, which are used for training the model.\n",
    "\n",
    "**Parameters**\n",
    "- **sequence:** A list of word indices.\n",
    "- **vocabulary_size:** The total number of words in the vocabulary.\n",
    "- **window_size:** The maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "**Returns**\n",
    "- **pairs:** A list of tuples where each tuple consists of a target word and a context word.\n",
    "- **labels:** A list of labels (not commonly used in Keras implementation).\n",
    "\n",
    "- (word, word in the same window), with label 1 (positive samples).\n",
    "- (word, random word from the vocabulary), with label 0 (negative samples).\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider a sentence \"The quick brown fox jumps over the lazy dog.\" After tokenization and indexing, the sequence might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5aba0e89-a282-4c1b-8d18-94aa5c9025dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cbcc2-d4eb-4923-b83c-1f5d755a1093",
   "metadata": {},
   "source": [
    "Here, each number corresponds to a word index in the vocabulary. Let's generate skip-gram pairs with window_size=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "10b24fff-60af-4caa-9dc4-30a4b7741866",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10\n",
    "window_size     = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "922d2fa4-175f-484d-b813-dd2e69cb636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skip-gram pairs\n",
    "pairs, labels = skipgrams(sequence, vocabulary_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef42203e-2d09-47b7-b287-d7141804bf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target  context  label\n",
       "0        6        5      1\n",
       "1        1        3      1\n",
       "2        6        8      1\n",
       "3        5        3      0\n",
       "4        5        7      0\n",
       "5        2        3      1\n",
       "6        7        9      1\n",
       "7        1        2      1\n",
       "8        6        6      0\n",
       "9        3        1      1\n",
       "10       4        5      1\n",
       "11       9        7      1\n",
       "12       5        7      1\n",
       "13       6        1      0\n",
       "14       4        2      0\n",
       "15       6        7      0\n",
       "16       7        9      0\n",
       "17       5        9      0\n",
       "18       3        1      0\n",
       "19       9        9      0\n",
       "20       5        7      0\n",
       "21       2        1      0\n",
       "22       9        5      0\n",
       "23       4        9      0\n",
       "24       8        3      0\n",
       "25       6        6      0\n",
       "26       3        5      1\n",
       "27       1        9      0\n",
       "28       6        4      1\n",
       "29       8        9      0\n",
       "30       7        8      0\n",
       "31       1        1      0\n",
       "32       2        4      1\n",
       "33       4        7      0\n",
       "34       3        4      0\n",
       "35       4        2      1\n",
       "36       7        8      0\n",
       "37       8        6      1\n",
       "38       5        6      1\n",
       "39       4        3      1\n",
       "40       3        2      0\n",
       "41       7        6      1\n",
       "42       2        1      1\n",
       "43       7        4      0\n",
       "44       8        9      1\n",
       "45       3        4      1\n",
       "46       4        6      1\n",
       "47       2        4      0\n",
       "48       5        3      1\n",
       "49       9        8      1\n",
       "50       2        5      0\n",
       "51       8        7      1\n",
       "52       7        8      1\n",
       "53       3        2      1\n",
       "54       7        5      1\n",
       "55       4        1      0\n",
       "56       6        7      1\n",
       "57       8        1      0\n",
       "58       3        6      0\n",
       "59       5        4      1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pairs and labels to DataFrame\n",
    "df = pd.DataFrame(pairs, columns=['target', 'context'])\n",
    "df['label'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04d393-8e91-4e82-883e-c0fca8cbcf02",
   "metadata": {},
   "source": [
    "... back to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "464a45d9-b250-44d7-91b0-c97ccd1ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(seq, \n",
    "                        vocabulary_size= vocab_size, \n",
    "                        window_size    = 2) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d92bb3f-d81a-4fa8-a4e2-bb00a94dccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of skip-grams and extract targets and contexts\n",
    "pairs = [pair for skip_gram in skip_grams for pair in skip_gram[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c381b2fe-38fe-4933-ba52-c68e658c0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, contexts = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46c5bd0f-875c-4d06-a256-699248ff5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets  = np.array(targets,  dtype=\"int32\")\n",
    "contexts = np.array(contexts, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30d47cd7-dbdf-4584-a5bd-2b2a770fd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a Simple Neural Network for Word2Vec Using Keras\n",
    "embedding_dim = 10  # Size of the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04530f00-8259-433e-a4bb-ab19e447822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and embedding layers\n",
    "input_target  = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, \n",
    "                      embedding_dim, \n",
    "                      input_length=1, name=\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad214f30-ea88-447b-85ab-267d4b64f47d",
   "metadata": {},
   "source": [
    "#### look up embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06ab4451-578d-4430-8758-dabc7d235206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the dense vector representation for each of these indices.\n",
    "target = embedding(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "53500eea-0da7-44dc-b865-d23e8bc39766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d364d1d-27cc-4001-878a-ef48a8a83a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Flatten()(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ead29151-139e-4014-b832-6d13cba02471",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = embedding(input_context)\n",
    "context = Flatten()(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1a7a6b1-cac3-428f-9569-6f9356af98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dot product of the embeddings\n",
    "# The dot product measures the similarity between the target and context word embeddings. \n",
    "# It gives a scalar value representing how closely related the two word vectors are.\n",
    "dot_product = Dot(axes=1)([target, context])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "83f7e2f3-2b96-400f-aaf1-619416f33da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to print dots\n",
    "class DotProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f51454-2f4d-4bde-8ec5-b3566927278c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1e430-9075-49b8-88c0-ed24ffef682f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1cac6a0e-7a44-4092-b8cc-eba9c819b5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with dot printing and capture history\n",
    "labels = np.ones(len(targets))  # Labels are all 1 for positive samples\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([targets, contexts], labels, epochs=100, verbose=0, callbacks=[DotProgress()])\n",
    "\n",
    "# Print new lines after dots\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bf6ef345-836e-41bd-9596-bdcb8f25238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss = 0.00018932\n",
      "Epoch 2: Training loss = 0.00013609\n",
      "Epoch 3: Training loss = 0.00010041\n",
      "Epoch 4: Training loss = 0.00007747\n",
      "Epoch 5: Training loss = 0.00006164\n",
      "Epoch 6: Training loss = 0.00005026\n",
      "Epoch 7: Training loss = 0.00004179\n",
      "Epoch 8: Training loss = 0.00003543\n",
      "Epoch 9: Training loss = 0.00003042\n",
      "Epoch 10: Training loss = 0.00002642\n",
      "Epoch 11: Training loss = 0.00002314\n",
      "Epoch 12: Training loss = 0.00002051\n",
      "Epoch 13: Training loss = 0.00001831\n",
      "Epoch 14: Training loss = 0.00001640\n",
      "Epoch 15: Training loss = 0.00001482\n",
      "Epoch 16: Training loss = 0.00001347\n",
      "Epoch 17: Training loss = 0.00001228\n",
      "Epoch 18: Training loss = 0.00001125\n",
      "Epoch 19: Training loss = 0.00001036\n",
      "Epoch 20: Training loss = 0.00000956\n",
      "Epoch 21: Training loss = 0.00000885\n",
      "Epoch 22: Training loss = 0.00000822\n",
      "Epoch 23: Training loss = 0.00000766\n",
      "Epoch 24: Training loss = 0.00000715\n",
      "Epoch 25: Training loss = 0.00000670\n",
      "Epoch 26: Training loss = 0.00000628\n",
      "Epoch 27: Training loss = 0.00000591\n",
      "Epoch 28: Training loss = 0.00000556\n",
      "Epoch 29: Training loss = 0.00000525\n",
      "Epoch 30: Training loss = 0.00000496\n",
      "Epoch 31: Training loss = 0.00000470\n",
      "Epoch 32: Training loss = 0.00000446\n",
      "Epoch 33: Training loss = 0.00000423\n",
      "Epoch 34: Training loss = 0.00000402\n",
      "Epoch 35: Training loss = 0.00000383\n",
      "Epoch 36: Training loss = 0.00000365\n",
      "Epoch 37: Training loss = 0.00000348\n",
      "Epoch 38: Training loss = 0.00000333\n",
      "Epoch 39: Training loss = 0.00000318\n",
      "Epoch 40: Training loss = 0.00000305\n",
      "Epoch 41: Training loss = 0.00000292\n",
      "Epoch 42: Training loss = 0.00000280\n",
      "Epoch 43: Training loss = 0.00000269\n",
      "Epoch 44: Training loss = 0.00000258\n",
      "Epoch 45: Training loss = 0.00000248\n",
      "Epoch 46: Training loss = 0.00000238\n",
      "Epoch 47: Training loss = 0.00000230\n",
      "Epoch 48: Training loss = 0.00000221\n",
      "Epoch 49: Training loss = 0.00000213\n",
      "Epoch 50: Training loss = 0.00000206\n",
      "Epoch 51: Training loss = 0.00000198\n",
      "Epoch 52: Training loss = 0.00000192\n",
      "Epoch 53: Training loss = 0.00000185\n",
      "Epoch 54: Training loss = 0.00000179\n",
      "Epoch 55: Training loss = 0.00000173\n",
      "Epoch 56: Training loss = 0.00000167\n",
      "Epoch 57: Training loss = 0.00000162\n",
      "Epoch 58: Training loss = 0.00000157\n",
      "Epoch 59: Training loss = 0.00000152\n",
      "Epoch 60: Training loss = 0.00000148\n",
      "Epoch 61: Training loss = 0.00000143\n",
      "Epoch 62: Training loss = 0.00000139\n",
      "Epoch 63: Training loss = 0.00000135\n",
      "Epoch 64: Training loss = 0.00000131\n",
      "Epoch 65: Training loss = 0.00000127\n",
      "Epoch 66: Training loss = 0.00000124\n",
      "Epoch 67: Training loss = 0.00000120\n",
      "Epoch 68: Training loss = 0.00000117\n",
      "Epoch 69: Training loss = 0.00000114\n",
      "Epoch 70: Training loss = 0.00000111\n",
      "Epoch 71: Training loss = 0.00000108\n",
      "Epoch 72: Training loss = 0.00000105\n",
      "Epoch 73: Training loss = 0.00000102\n",
      "Epoch 74: Training loss = 0.00000100\n",
      "Epoch 75: Training loss = 0.00000097\n",
      "Epoch 76: Training loss = 0.00000095\n",
      "Epoch 77: Training loss = 0.00000092\n",
      "Epoch 78: Training loss = 0.00000090\n",
      "Epoch 79: Training loss = 0.00000088\n",
      "Epoch 80: Training loss = 0.00000086\n",
      "Epoch 81: Training loss = 0.00000084\n",
      "Epoch 82: Training loss = 0.00000082\n",
      "Epoch 83: Training loss = 0.00000080\n",
      "Epoch 84: Training loss = 0.00000078\n",
      "Epoch 85: Training loss = 0.00000076\n",
      "Epoch 86: Training loss = 0.00000075\n",
      "Epoch 87: Training loss = 0.00000073\n",
      "Epoch 88: Training loss = 0.00000071\n",
      "Epoch 89: Training loss = 0.00000070\n",
      "Epoch 90: Training loss = 0.00000068\n",
      "Epoch 91: Training loss = 0.00000067\n",
      "Epoch 92: Training loss = 0.00000065\n",
      "Epoch 93: Training loss = 0.00000064\n",
      "Epoch 94: Training loss = 0.00000063\n",
      "Epoch 95: Training loss = 0.00000061\n",
      "Epoch 96: Training loss = 0.00000060\n",
      "Epoch 97: Training loss = 0.00000059\n",
      "Epoch 98: Training loss = 0.00000057\n",
      "Epoch 99: Training loss = 0.00000056\n",
      "Epoch 100: Training loss = 0.00000055\n"
     ]
    }
   ],
   "source": [
    "# Access and print training loss\n",
    "losses = history.history['loss']\n",
    "for epoch, loss in enumerate(losses):\n",
    "    print(f'Epoch {epoch + 1}: Training loss = {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a278cd7c-49b1-4aba-8fa7-5f8e07dfe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract Embeddings\n",
    "embedding_layer = model.get_layer('embedding')\n",
    "embedding_weights = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "158ffc00-7264-49e0-a130-df6c9fde83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 10)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights.shape)  # Should be (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b3fa5e3-2162-4da5-ba59-cde61a4a3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(index2word))  # Should be vocab_size - 1 if vocab_size includes padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bcd445da-6fe4-4d19-ada3-3dff29c136d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006641</td>\n",
       "      <td>-0.035035</td>\n",
       "      <td>-0.022339</td>\n",
       "      <td>-0.011309</td>\n",
       "      <td>-0.033168</td>\n",
       "      <td>0.043048</td>\n",
       "      <td>-0.017726</td>\n",
       "      <td>0.013206</td>\n",
       "      <td>-0.031161</td>\n",
       "      <td>-0.025686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.588330</td>\n",
       "      <td>-0.328001</td>\n",
       "      <td>-0.732139</td>\n",
       "      <td>-0.754788</td>\n",
       "      <td>0.750999</td>\n",
       "      <td>-0.484599</td>\n",
       "      <td>-0.525613</td>\n",
       "      <td>-0.770547</td>\n",
       "      <td>0.185323</td>\n",
       "      <td>0.755284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.512723</td>\n",
       "      <td>-0.632475</td>\n",
       "      <td>-0.602117</td>\n",
       "      <td>-0.701690</td>\n",
       "      <td>0.671994</td>\n",
       "      <td>0.603150</td>\n",
       "      <td>-0.700471</td>\n",
       "      <td>-0.625682</td>\n",
       "      <td>-0.643193</td>\n",
       "      <td>0.745278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.368420</td>\n",
       "      <td>-0.804940</td>\n",
       "      <td>0.357299</td>\n",
       "      <td>-0.591159</td>\n",
       "      <td>0.769499</td>\n",
       "      <td>0.486790</td>\n",
       "      <td>-0.759999</td>\n",
       "      <td>-0.812508</td>\n",
       "      <td>-0.619820</td>\n",
       "      <td>0.806190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.343359</td>\n",
       "      <td>-0.758530</td>\n",
       "      <td>-0.313048</td>\n",
       "      <td>-0.668963</td>\n",
       "      <td>0.750749</td>\n",
       "      <td>-0.326777</td>\n",
       "      <td>-0.606630</td>\n",
       "      <td>-0.678244</td>\n",
       "      <td>-0.142103</td>\n",
       "      <td>0.717890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.006641 -0.035035 -0.022339 -0.011309 -0.033168  0.043048 -0.017726   \n",
       "1 -0.588330 -0.328001 -0.732139 -0.754788  0.750999 -0.484599 -0.525613   \n",
       "2  0.512723 -0.632475 -0.602117 -0.701690  0.671994  0.603150 -0.700471   \n",
       "3  0.368420 -0.804940  0.357299 -0.591159  0.769499  0.486790 -0.759999   \n",
       "4 -0.343359 -0.758530 -0.313048 -0.668963  0.750749 -0.326777 -0.606630   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.013206 -0.031161 -0.025686  \n",
       "1 -0.770547  0.185323  0.755284  \n",
       "2 -0.625682 -0.643193  0.745278  \n",
       "3 -0.812508 -0.619820  0.806190  \n",
       "4 -0.678244 -0.142103  0.717890  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the embeddings in a DataFrame\n",
    "embeddings_df = pd.DataFrame(embedding_weights)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "524f26d3-f1d2-4f8c-b5f9-2ff0281983dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Function to Retrieve Embedding for a Given Word\n",
    "def get_embedding(word):\n",
    "    idx = word2index.get(word)\n",
    "    if idx is not None:\n",
    "        return embedding_weights[idx]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "609d0ce6-8f70-498b-8dd7-c5c5c71f5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Function to Find Most Similar Tokens\n",
    "def most_similar(word, top_n=5):\n",
    "    target_vector = get_embedding(word)\n",
    "    if target_vector is None:\n",
    "        return None\n",
    "\n",
    "    similarities = np.dot(embedding_weights, target_vector)\n",
    "    similarities /= np.linalg.norm(embedding_weights, axis=1)  # Normalize\n",
    "    similarities /= np.linalg.norm(target_vector)  # Normalize\n",
    "\n",
    "    # Get the top_n most similar words\n",
    "    most_similar_indices = similarities.argsort()[-top_n-1:][::-1]\n",
    "    most_similar_words = [(index2word[i], similarities[i]) for i in most_similar_indices if i != word2index[word]]\n",
    "    return most_similar_words[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6c57566e-96d4-4bdf-88f8-93d36453b7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3433594  -0.7585301  -0.31304765 -0.66896296  0.7507492  -0.32677683\n",
      " -0.60662997 -0.6782444  -0.1421033   0.71789044]\n"
     ]
    }
   ],
   "source": [
    "# Example: Retrieve embedding for a word\n",
    "print(get_embedding(\"COVID-19\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30a437e4-0724-4fe3-8748-a0ea213febbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('creating', 0.9896402), ('Many', 0.9776088), ('mutates', 0.9723464), ('Research', 0.9714599), ('recovered', 0.9679285)]\n"
     ]
    }
   ],
   "source": [
    "# Example: Find most similar tokens to a word\n",
    "print(most_similar(\"COVID-19\", top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c7760-d9f4-49d6-96cb-ad1fe073fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
