{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b154103-ec70-4e04-8ad0-c2e2afe55e32",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "#### Word embeddings \n",
    "--------------------------\n",
    "\n",
    "- Objective\n",
    "\n",
    "    - build a binary classification model\n",
    "    - perform sentiment analysis on IMDB dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f734635-06ac-44cf-a623-7e568abb808c",
   "metadata": {},
   "source": [
    "**Data Download and Extraction:**\n",
    "\n",
    "Downloads a sentiment analysis dataset (IMDb reviews) from a specified URL.\n",
    "Extracts the dataset from the downloaded tar.gz file.\n",
    "\n",
    "**Data Preparation:**\n",
    "\n",
    "Creates directories for training and validation data.\n",
    "Loads the training data using text_dataset_from_directory from TensorFlow, splitting it into training and validation subsets.\n",
    "\n",
    "**Data Preprocessing and Optimization:**\n",
    "\n",
    "Defines the custom_standardization function to perform text preprocessing, converting text to lowercase and stripping HTML break tags.\n",
    "Uses the TextVectorization layer to normalize, split, and map strings to integers, adapting it to the training data.\n",
    "Sets up the AUTOTUNE constant and applies caching and prefetching to the training and validation datasets for optimized performance.\n",
    "\n",
    "**Model Definition:**\n",
    "\n",
    "Creates a text classification model using TensorFlow's Keras API.\n",
    "Comprises layers for text vectorization, embedding, global average pooling, and two dense layers for classification.\n",
    "Specifies the vocabulary size, sequence length, and embedding dimension.\n",
    "\n",
    "**Model Training:**\n",
    "\n",
    "Compiles and trains the defined model on the preprocessed training dataset.\n",
    "Utilizes the fit method with specified parameters such as training data, validation data, number of epochs, and callbacks.\n",
    "\n",
    "**TensorBoard Callback:**\n",
    "\n",
    "There is a reference to a tensorboard_callback, which suggests the usage of TensorBoard for model training visualization. However, the instantiation and definition of this callback are not provided in the provided code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d004f780-6eba-4dc0-bf4c-8e4be5d08dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ebf46-0955-4c0e-9fdb-e72c7e95ea3a",
   "metadata": {},
   "source": [
    "#### Download the IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f9b937-4ed9-44ee-bb6c-618d6c297f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 54s\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# 82 MB file\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", \n",
    "                                  url,\n",
    "                                  untar       = True, \n",
    "                                  cache_dir   = r'D:\\AI-DATASETS\\02-MISC-large\\keras\\datasets',\n",
    "                                  cache_subdir= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7359e98-c04b-469b-b06d-c0fa7051f3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b9f93c7-c3d8-4b0e-bdc2-4c54b1a11237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5623f-776f-49ec-879a-4302664a6866",
   "metadata": {},
   "source": [
    "**train/ directory**\n",
    "\n",
    "- `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "126327bc-ac7c-48ee-a4f9-e91e1f5c41d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75268314-adf6-448e-9dbc-ae3124b0f56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb\\\\train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed687bce-2711-42fb-b3a3-58db934cd3f6",
   "metadata": {},
   "source": [
    "The train directory also has additional folders which should be removed before creating training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bd497ae-6cce-4c62-a0d1-af99d836e29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.14 s\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb9fcf-19ca-4638-8049-434b16e29282",
   "metadata": {},
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.utils.text_dataset_from_directory`.\n",
    "\n",
    "Use the train directory to create both train and validation datasets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe6de88d-39b8-4d0f-a132-3b8e99c2ce2f",
   "metadata": {},
   "source": [
    "If your directory structure is:\n",
    "\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```\n",
    "\n",
    "Then calling `text_dataset_from_directory(main_directory,\n",
    "labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n",
    "texts from the subdirectories `class_a` and `class_b`, together with labels\n",
    "0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06c9a161-c0f4-46bc-9633-9db1ea58d5af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "CPU times: total: 3.34 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1024\n",
    "seed       = 123\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "                    train_dir, \n",
    "                    batch_size      = batch_size, \n",
    "                    validation_split= 0.2,\n",
    "                    subset          = 'training', \n",
    "                    seed            = seed)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "                    train_dir, \n",
    "                    batch_size      = batch_size, \n",
    "                    validation_split= 0.2,\n",
    "                    subset          = 'validation', \n",
    "                    seed            = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf132e-9d4b-4cdf-a2f7-11afa0e63281",
   "metadata": {},
   "source": [
    "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d4bff4-6828-4027-a533-ea2bf352d85c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b\"Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have stopped watching half way through. But I thought it might get better. It Didn't. Anyone who actually enjoyed this movie is one seriously sick and twisted individual. No wonder us Australians/New Zealanders have a terrible reputation when it comes to making movies. Everything about this movie is horrible, from the acting to the editing. I don't even normally write reviews on here, but in this case I'll make an exception. I only wish someone had of warned me before I hired this catastrophe\"\n",
      "\n",
      "1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n",
      "\n",
      "CPU times: total: 1.53 s\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3312b2-b7c5-42f0-8fe4-36c1d476b80d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sets the variable AUTOTUNE to the special value tf.data.AUTOTUNE, \n",
    "# which is a constant used in TensorFlow to dynamically tune the performance of \n",
    "# operations based on the available resources.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# caches the elements of the dataset in memory\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c4d38-8d58-4a9b-b560-4a841618b22d",
   "metadata": {},
   "source": [
    "#### how are we going to create embeddings\n",
    "\n",
    "Given review text : \"Some movies just leave me speechless.. any other cast-member of SNL\"\n",
    "\n",
    "1. first tokenize the text into words\n",
    "2. assign unique integer number (think like a code) to every word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a96e9-bc44-4ab2-83ea-6c38b82de376",
   "metadata": {},
   "source": [
    "#### Using the Embedding layer\n",
    "\n",
    "- The Embedding layer serves as a lookup table, associating integer indices with dense vectors that represent the embeddings of specific words.\n",
    "- It can be compared to a parameterized table where each word is assigned a unique dense vector.\n",
    "- The dimensionality or width of the embedding is a tunable parameter, allowing experimentation to find an optimal setting for a given problem.\n",
    "- Similar to adjusting the number of neurons in a Dense layer, modifying the embedding dimensionality enables fine-tuning for improved model performance.\n",
    "- Experimenting with different embedding dimensions helps determine the most effective representation of words in the context of a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c523b85d-ea3a-4d62-a4d1-d202f396dd22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=1000, output_dim=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6294364-0fa9-4dd6-8a8f-d8720c35a755",
   "metadata": {},
   "source": [
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d0c1e72-23eb-483c-8f98-9a4522e14c53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00515721, -0.01837242, -0.02512432,  0.0366399 , -0.04934866],\n",
       "       [-0.02547615, -0.01515649, -0.03694643,  0.03688324, -0.03789264],\n",
       "       [-0.04340212,  0.03707488,  0.01986596, -0.00517799,  0.00846215]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47f75d-f196-4c8f-8284-8be7708f0f85",
   "metadata": {},
   "source": [
    "**for text data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11992e-0277-4de5-9fce-d68580b44125",
   "metadata": {},
   "source": [
    "- For text or sequence-related problems, the Embedding layer in neural networks accepts a 2D tensor of integers with a shape of (`samples`, `sequence_length`).\n",
    "\n",
    "- Each entry in this tensor represents a sequence of integers, allowing the layer to handle variable-length sequences effectively.\n",
    "\n",
    "- Batches with different shapes, such as (32, 10) or (64, 15), can be fed into the Embedding layer, where 32 or 64 represents the number of sequences in the batch, and 10 or 15 is the length of each sequence.\n",
    "\n",
    "- The resulting tensor from the Embedding layer has one additional axis compared to the input. The embedding vectors are aligned along this new last axis.\n",
    "- If a batch with a shape of (2, 3) is passed to the Embedding layer, the output tensor will be of shape (2, 3, N), where N represents the dimensionality of the embedding space. \n",
    "- The embeddings for each integer in the input sequences are aligned along the new axis, preserving the sequence structure.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e0d7c6-8324-45d4-876b-d53c25197d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf791148-dbcc-4675-8f80-1e67f77696a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], \n",
    "                                      [3, 4, 5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97024270-1d48-4342-b187-46678bdce98f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 5), dtype=float32, numpy=\n",
       "array([[[-0.03659021,  0.01373824, -0.01638372,  0.014025  ,  0.00111081],\n",
       "        [ 0.00515721, -0.01837242, -0.02512432,  0.0366399 , -0.04934866],\n",
       "        [-0.02547615, -0.01515649, -0.03694643,  0.03688324, -0.03789264]],\n",
       "\n",
       "       [[-0.04340212,  0.03707488,  0.01986596, -0.00517799,  0.00846215],\n",
       "        [ 0.02256364,  0.00312041, -0.03689418,  0.02967275,  0.00882412],\n",
       "        [-0.01595887, -0.02671605, -0.01254153, -0.03929228,  0.01542492]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710428fa-0f8f-4beb-b430-4f3b9039f231",
   "metadata": {},
   "source": [
    "- When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (`samples`, `sequence_length`, `embedding_dimensionality`). \n",
    "- To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. \n",
    "- You could use an RNN, Attention, or pooling layer before passing it to a Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdae533-48d6-4014-b7f1-37c83c41a61b",
   "metadata": {},
   "source": [
    "#### Text preprocessing\n",
    "Next, define the dataset preprocessing steps required for your sentiment classification model. \n",
    "\n",
    "Initialize a `TextVectorization layer` with the desired parameters to vectorize movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d4d6b22-6ac6-4811-95a7-bfdaba1c6eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a64096a3-288f-4b90-8f12-4297b7689546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "train_texts = [\"This is a sample sentence.\", \n",
    "               \"Another example sentence.\", \n",
    "               \"TensorFlow is great!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab608003-d6c5-4b8b-a5d3-b321f91e9187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "text_vectorizer = TextVectorization(max_tokens            = 100, \n",
    "                                    output_mode           = 'int', \n",
    "                                    output_sequence_length= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e73d9ef8-82ee-4248-8215-c070cfffeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapt the layer to your training text data\n",
    "text_vectorizer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a2f6d6f-0428-4480-a0a8-a4f10b3d4a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform input text into numerical vectors\n",
    "input_texts = [\"Sample sentence for testing.\", \"TensorFlow example.\"]\n",
    "numerical_vectors = text_vectorizer(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b620d3b-c51d-4ace-bda3-0421a2b98607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts:\n",
      "['This is a sample sentence.', 'Another example sentence.', 'TensorFlow is great!']\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Original texts:\")\n",
    "print(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fedf545f-b574-4655-89bd-92aa07c15e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical vectors:\n",
      "[[6 2 1 1 0]\n",
      " [5 8 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumerical vectors:\")\n",
    "print(numerical_vectors.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148bc74-cf1a-4577-8076-a35729a1b009",
   "metadata": {},
   "source": [
    "...back to code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5185769d-b716-4c88-90f2-263d52c003ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase     = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3376a7fe-c48c-44ab-8be8-79d5770209f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size      = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f883ddf1-1942-4a8f-8559-84a0402603ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. \n",
    "# Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "                        standardize           = custom_standardization,\n",
    "                        max_tokens            = vocab_size,\n",
    "                        output_mode           = 'int',\n",
    "                        output_sequence_length= sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b002dc1-5dbc-4e5e-8d90-0341028172bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000002384CE86160> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x000002384CE86160>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x000002384CE86160> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x000002384CE86160>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: total: 34.6 s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d753b64-a51a-4861-b9a8-154428fc7a0c",
   "metadata": {},
   "source": [
    "#### Constructing a Sentiment Classification Model\n",
    "\n",
    "- Utilize the Keras Sequential API to establish a sentiment classification model, specifically adopting a `Continuous Bag of Words` style.\n",
    "\n",
    "- The TextVectorization layer plays a crucial role in transforming strings into vocabulary indices. \n",
    "\n",
    "- After initializing vectorize_layer as a TextVectorization layer and building its vocabulary through the adaptation process on text_ds, it becomes a fundamental component as the initial layer in the end-to-end classification model. \n",
    "\n",
    "- This layer efficiently feeds transformed strings into the subsequent Embedding layer.\n",
    "\n",
    "- The `Embedding layer` takes the `integer-encoded vocabulary` and retrieves the corresponding `embedding vector` for each word-index. \n",
    "\n",
    "- These vectors evolve and improve as the model undergoes training, adding an extra dimension to the output array. The resultant dimensions following this operation are (batch, sequence, embedding).\n",
    "\n",
    "- To obtain a fixed-length output vector for each example, the model incorporates the GlobalAveragePooling1D layer. \n",
    "\n",
    "- This layer achieves this by averaging over the sequence dimension, ensuring the model can handle inputs of varying lengths in a straightforward manner.\n",
    "\n",
    "- The fixed-length output vector then progresses through a fully-connected (Dense) layer featuring 16 hidden units.\n",
    "\n",
    "- Concluding the architecture, the last layer establishes a dense connection with a single output node.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "69ed618c-0252-4e47-a941-e750d2f16ab4",
   "metadata": {},
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "                        standardize           = custom_standardization,\n",
    "                        max_tokens            = vocab_size,\n",
    "                        output_mode           = 'int',\n",
    "                        output_sequence_length= sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b70847aa-c098-45c8-aeb1-71681af858c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'), # optional\n",
    "  Dense(1)                      # binary\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88a47-78f8-4c12-9e7b-8cb001678994",
   "metadata": {},
   "source": [
    "#### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4231943e-b485-49b6-b37a-b3172752a157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c28290ed-5be2-4805-ab96-ba8cd95eff70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "167e045a-1e70-4614-96ed-6437a50990d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_3                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_3                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff1210-6b07-4f59-9c9f-f35234199d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4985 - loss: 0.6915"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6538b547-d206-4e3a-96d7-171a4a8902c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #docs_infra: no_execute\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca3c79-b50a-479b-a73e-a7fd07695858",
   "metadata": {},
   "source": [
    "#### Retrieve the trained word embeddings and save them to disk\n",
    "Next, retrieve the word embeddings learned during training. \n",
    "\n",
    "The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape (vocab_size, embedding_dimension).\n",
    "\n",
    "Obtain the weights from the model using get_layer() and get_weights(). \n",
    "\n",
    "The get_vocabulary() function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6367a3f6-8982-4a01-a2e3-b283402134b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37aad583-4325-4439-b87d-e4b059af41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c7256-e86f-4431-a3b1-e11a6f6b64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
