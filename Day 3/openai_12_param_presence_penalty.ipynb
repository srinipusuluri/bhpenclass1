{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb1abf-78c4-4e49-b962-15199bb520d4",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Presence penalty\n",
    "\n",
    "- default = 0\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cc60a-8f4a-4681-8edf-ede2b0b3598a",
   "metadata": {},
   "source": [
    "| **Aspect**                        | **Without Presence Penalty**                                                                                                     | **With Presence Penalty**                                                                                                         | **Observation**                                                                                                                    |\n",
    "|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Example 1**                     | \"A sunny day at the beach is a great day. The sun shines brightly, and the sun makes everyone feel happy. The beach is filled with people enjoying the sun. People love to sunbathe on the beach. The beach has beautiful sand, and the waves are calm. A sunny day is perfect for swimming in the ocean. Everyone enjoys the sun and the beach.\" | \"A sunny day at the beach is a great day. Bright sunshine fills the air, and many people enjoy their time by the water. Visitors love to relax on the soft sand, while some play games near the shore. The gentle waves invite swimmers, creating a joyful atmosphere that makes for perfect memories.\" | In the first example (without a presence penalty), the model repeatedly uses the phrases \"sun,\" \"beach,\" and \"people,\" leading to a monotonous output. In the second example (with a presence penalty), the model avoids repeating these phrases and uses varied expressions, resulting in a more engaging description. |\n",
    "| **Example 2**                     | \"A dog is a loyal pet. The dog is a great companion for many people. Dogs are known for their playful nature. A dog needs exercise and love. Dogs can be trained to do tricks. Many people have dogs. A dog is not just a pet; it is a friend.\" | \"A dog is often regarded as a loyal companion. Many enjoy the playful nature of these animals. Regular exercise and affection are vital for their well-being. Some dogs are trained to perform tricks, making them entertaining additions to any household. They provide friendship and support to their owners.\" | In the first example (without a presence penalty), the model generates repetitive content with words like \"dog\" and \"dogs\" being used excessively. In the second example (with a presence penalty), the model produces more dynamic language, improving the coherence and appeal of the text. |\n",
    "| **Summary**                       | Without presence penalty, the model generates repetitive phrases, leading to a less engaging and monotonous text.                     | With presence penalty, the model avoids repeating phrases, resulting in varied and engaging language.                              | In contexts where diversity and engagement are crucial, using a presence penalty can significantly enhance the quality of the generated output.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26d74b-bec8-4b30-a4a9-5daee77bfdac",
   "metadata": {},
   "source": [
    "#### 1. Purpose\n",
    "The primary goal of the presence penalty is to `reduce the likelihood of the model repeating the same words or phrases` within a given output. This is particularly useful in creative writing, dialogue generation, and any context where varied language is preferred.\n",
    "\n",
    "#### 2. Mechanism\n",
    "- **Token Tracking**: As the model generates text, it keeps track of all the tokens (words or phrases) that have already been used in the current output.\n",
    "- **Penalty Application**: When generating the next token, the model applies a penalty to any token that has already appeared. This penalty reduces the token's probability score, making it less likely to be chosen as the next word.\n",
    "- **Dynamic Adjustment**: The presence penalty can be adjusted based on the user's preferences. A higher penalty makes it even less likely for previously used tokens to be selected again.\n",
    "\n",
    "#### 3. Implementation\n",
    "- **Parameter Setting**: In API calls to models like OpenAI's, you can set the presence penalty parameter (usually between 0 and 2, with 0 being no penalty and 2 being a strong penalty).\n",
    "- **Effect on Output**: By setting a presence penalty, the model is encouraged to use synonyms, alternate expressions, or different sentence structures to avoid repeating words, leading to richer and more varied text.\n",
    "\n",
    "#### 4. Example of Functionality\n",
    "- **Without Presence Penalty**:\n",
    "  - Prompt: \"The cat sat on the mat.\"\n",
    "  - Output: \"The cat looked around. The cat saw a bird.\"\n",
    "  \n",
    "- **With Presence Penalty**:\n",
    "  - Prompt: \"The cat sat on the mat.\"\n",
    "  - Output: \"The feline looked around. It spotted a bird.\"\n",
    "\n",
    "#### 5. Use Cases\n",
    "- **Creative Writing**: Enhances storytelling by preventing repetitive language.\n",
    "- **Dialogue Generation**: Produces more natural conversations by varying vocabulary.\n",
    "- **Content Creation**: Ensures engaging content in articles, blogs, and other writing forms.\n",
    "\n",
    "#### Summary\n",
    "The presence penalty is an effective tool for enhancing the diversity and creativity of generated text by discouraging the model from repeating words or phrases. By adjusting the presence penalty, users can control the level of variety in the output, leading to higher quality and more engaging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4172e566-8859-4f45-a8f5-cc56beecc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566dc9fc-6d75-494e-b89a-0956e8d24e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.45.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a12d99-a779-4357-9375-78a570015250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e99c6f-b0ac-4ee8-bdaa-f01eccd3587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d984ff-adba-4593-83f0-ae19b043f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text \n",
    "def generate_story(prompt, presence_penalty=0, max_tokens=200):\n",
    "    response = client.chat.completions.create(\n",
    "        model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "        messages   = [\n",
    "            {\"role\": \"assistant\", \"content\": f'Generate your answers in {max_tokens} words.'},\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        max_tokens       = max_tokens,\n",
    "        presence_penalty = presence_penalty \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ab40bf5-4816-493c-9e69-8c5d6887d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'advantages of LLMs in business '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "max_tokens = 1000\n",
    "prompt = f'''advantages of LLMs in business '''\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61da3a3b-72da-4d1f-95c7-6f4a99bd4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty = 0\n"
     ]
    }
   ],
   "source": [
    "# Low presence penalty\n",
    "pres_penalty = 0\n",
    "\n",
    "print(f\"Presence Penalty = {pres_penalty}\")\n",
    "#print(generate_story(prompt, presence_penalty=pres_penalty))\n",
    "\n",
    "text = generate_story(prompt, presence_penalty=pres_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "613eea8f-e15d-4e8d-8271-ac38685c6ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Obtaining a Master of Laws (LLM) degree can provide several significant advantages for professionals in the business world. Firstly, an LLM can enhance a person's legal knowledge and expertise, enabling them to better navigate complex legal issues within their industry. This deeper understanding of the law can help businesses minimize legal risks and ensure compliance with regulations, ultimately contributing to the company's overall success.\\n\\nAdditionally, an LLM can open up new career opportunities within the business sector. Specializing in areas such as international business law, intellectual property, or taxation through an LLM program can make an individual more marketable to potential employers seeking expertise in these specific areas. The advanced legal skills gained through an LLM can also lead to increased job responsibilities and potentially higher salaries.\\n\\nMoreover, the networking opportunities provided by an LLM program can be invaluable for business professionals. Connecting with fellow students, professors, and industry professionals can lead to collaborations, partnerships, and new business ventures that may not have been possible without the L\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f9a8604-8285-4c44-9cd1-4b79fa6f6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_cat_variations(text):\n",
    "    # Define a pattern to match 'cat', 'cats', and 'cat's' (case insensitive)\n",
    "    pattern = r'\\b(LLM|LLMs|LLM\\'s)\\b'\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Count occurrences\n",
    "    counts = {\n",
    "        'cat': matches.count('LLM'),\n",
    "        'cats': matches.count('LLMs'),\n",
    "        \"cat's\": matches.count(\"LLM's\")\n",
    "    }\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f146e4f-5ade-4756-b8e3-5dd8c1d0a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 6, 'cats': 0, \"cat's\": 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cat_variations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62d75186-4d6e-4cec-ac0c-6339c514128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty = 2\n"
     ]
    }
   ],
   "source": [
    "# High presence penalty\n",
    "pres_penalty = 2\n",
    "\n",
    "print(f\"Presence Penalty = {pres_penalty}\")\n",
    "#print(generate_story(prompt, presence_penalty=pres_penalty))\n",
    "\n",
    "text = generate_story(prompt, presence_penalty=pres_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1235bb14-a2dc-4cbe-9687-1c7471edba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'cats': 6, \"cat's\": 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cat_variations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "809adff6-ecb9-4cef-a42e-78893f9bf32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LLMs (Limited Liability Companies) offer several advantages for businesses, making them a popular choice for entrepreneurs and small business owners. One key advantage is the limited liability protection that LLMs provide to their owners. This means that the personal assets of the owners are protected from being used to settle business debts or legal claims.\\n\\nAdditionally, LLMs have more flexibility in terms of management structure and profit distribution compared to other entity types like corporations. Owners can decide how they want to allocate profits and losses among themselves without being subject to strict corporate formalities. \\n\\nLLMs also offer tax benefits, as they are typically treated as pass-through entities for tax purposes. This means that profits and losses pass through to the individual owners' tax returns, avoiding double taxation at both the business and individual level.\\n\\nLastly, LLMs have a relatively simple setup process and fewer ongoing compliance requirements, making them easier and less costly to maintain than corporations. These advantages make LLMs an attractive option for many businesses looking to\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b4ddd-45df-460a-9630-5d367d1ff288",
   "metadata": {},
   "source": [
    "#### Frequency vs presence penalties\n",
    "\n",
    "| **Aspect**                 | **Frequency Penalty**                                                                 | **Presence Penalty**                                                                  |\n",
    "|----------------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| **Purpose**                | Reduces the likelihood of generating tokens based on how often they have been used.  | Discourages the generation of any tokens that have been used at least once.         |\n",
    "| **Implementation**         | Penalty based on the count of occurrences of a token in the current output.          | Binary check: a constant penalty is applied if a token has been used at least once. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f90e3-c743-4e64-a724-fe0f811dc3ce",
   "metadata": {},
   "source": [
    "#### 1. Token Probability Distribution\n",
    "In a language model, each token (word or symbol) has an associated probability that indicates how likely it is to be the next token given the preceding context. This probability is typically calculated using a softmax function.\n",
    "\n",
    "**Softmax Function**\n",
    "The softmax function converts raw scores (logits) into probabilities:\n",
    "\n",
    "$$\\Large [\n",
    "P(y_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "- $P(y_i) $ is the probability of token ( i ).\n",
    "- $( z_i ) $) is the logit score for token ( i ).\n",
    "- The denominator is the sum of exponentials of all logit scores, ensuring that the probabilities sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae521f-1586-4fd0-ac95-7d5398e4b5b3",
   "metadata": {},
   "source": [
    "#### 2. Applying the Presence Penalty\n",
    "The presence penalty modifies the logit score of tokens that have already appeared in the output to reduce their probability. This can be represented mathematically as:\n",
    "\n",
    "$$ \\Large [\n",
    "z_i' = z_i - P \\cdot \\text{presence\\_penalty}(y_i)\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "- \\( z_i' \\) is the modified logit score for token \\( i \\).\n",
    "- \\( P \\) is a constant that determines the strength of the penalty.\n",
    "- \\( \\text{presence\\_penalty}(y_i) \\) is a function that returns a penalty value (typically 1 or a scaling factor) for token \\( y_i \\) if it has already been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef724ff6-5a6a-4bc6-9d95-7a02a426d969",
   "metadata": {},
   "source": [
    "#### 4. Re-calculating Token Probabilities\n",
    "After applying the presence penalty, the new logits \\( z' \\) are used to calculate the updated probabilities using the softmax function:\n",
    "\n",
    "$$\\Large [\n",
    "P'(y_i) = \\frac{e^{z_i'}}{\\sum_{j} e^{z_j'}}\n",
    "]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6f222-1b62-4636-b520-777abdfed4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
