{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb1abf-78c4-4e49-b962-15199bb520d4",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Presence penalty\n",
    "\n",
    "- default = 0\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cc60a-8f4a-4681-8edf-ede2b0b3598a",
   "metadata": {},
   "source": [
    "| **Aspect**                        | **Without Presence Penalty**                                                                                                     | **With Presence Penalty**                                                                                                         | **Observation**                                                                                                                    |\n",
    "|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Example 1**                     | \"A sunny day at the beach is a great day. The sun shines brightly, and the sun makes everyone feel happy. The beach is filled with people enjoying the sun. People love to sunbathe on the beach. The beach has beautiful sand, and the waves are calm. A sunny day is perfect for swimming in the ocean. Everyone enjoys the sun and the beach.\" | \"A sunny day at the beach is a great day. Bright sunshine fills the air, and many people enjoy their time by the water. Visitors love to relax on the soft sand, while some play games near the shore. The gentle waves invite swimmers, creating a joyful atmosphere that makes for perfect memories.\" | In the first example (without a presence penalty), the model repeatedly uses the phrases \"sun,\" \"beach,\" and \"people,\" leading to a monotonous output. In the second example (with a presence penalty), the model avoids repeating these phrases and uses varied expressions, resulting in a more engaging description. |\n",
    "| **Example 2**                     | \"A dog is a loyal pet. The dog is a great companion for many people. Dogs are known for their playful nature. A dog needs exercise and love. Dogs can be trained to do tricks. Many people have dogs. A dog is not just a pet; it is a friend.\" | \"A dog is often regarded as a loyal companion. Many enjoy the playful nature of these animals. Regular exercise and affection are vital for their well-being. Some dogs are trained to perform tricks, making them entertaining additions to any household. They provide friendship and support to their owners.\" | In the first example (without a presence penalty), the model generates repetitive content with words like \"dog\" and \"dogs\" being used excessively. In the second example (with a presence penalty), the model produces more dynamic language, improving the coherence and appeal of the text. |\n",
    "| **Summary**                       | Without presence penalty, the model generates repetitive phrases, leading to a less engaging and monotonous text.                     | With presence penalty, the model avoids repeating phrases, resulting in varied and engaging language.                              | In contexts where diversity and engagement are crucial, using a presence penalty can significantly enhance the quality of the generated output.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26d74b-bec8-4b30-a4a9-5daee77bfdac",
   "metadata": {},
   "source": [
    "#### 1. Purpose\n",
    "The primary goal of the presence penalty is to `reduce the likelihood of the model repeating the same words or phrases` within a given output. This is particularly useful in creative writing, dialogue generation, and any context where varied language is preferred.\n",
    "\n",
    "#### 2. Mechanism\n",
    "- **Token Tracking**: As the model generates text, it keeps track of all the tokens (words or phrases) that have already been used in the current output.\n",
    "- **Penalty Application**: When generating the next token, the model applies a penalty to any token that has already appeared. This penalty reduces the token's probability score, making it less likely to be chosen as the next word.\n",
    "- **Dynamic Adjustment**: The presence penalty can be adjusted based on the user's preferences. A higher penalty makes it even less likely for previously used tokens to be selected again.\n",
    "\n",
    "#### 3. Implementation\n",
    "- **Parameter Setting**: In API calls to models like OpenAI's, you can set the presence penalty parameter (usually between 0 and 2, with 0 being no penalty and 2 being a strong penalty).\n",
    "- **Effect on Output**: By setting a presence penalty, the model is encouraged to use synonyms, alternate expressions, or different sentence structures to avoid repeating words, leading to richer and more varied text.\n",
    "\n",
    "#### 4. Example of Functionality\n",
    "- **Without Presence Penalty**:\n",
    "  - Prompt: \"The cat sat on the mat.\"\n",
    "  - Output: \"The cat looked around. The cat saw a bird.\"\n",
    "  \n",
    "- **With Presence Penalty**:\n",
    "  - Prompt: \"The cat sat on the mat.\"\n",
    "  - Output: \"The feline looked around. It spotted a bird.\"\n",
    "\n",
    "#### 5. Use Cases\n",
    "- **Creative Writing**: Enhances storytelling by preventing repetitive language.\n",
    "- **Dialogue Generation**: Produces more natural conversations by varying vocabulary.\n",
    "- **Content Creation**: Ensures engaging content in articles, blogs, and other writing forms.\n",
    "\n",
    "#### Summary\n",
    "The presence penalty is an effective tool for enhancing the diversity and creativity of generated text by discouraging the model from repeating words or phrases. By adjusting the presence penalty, users can control the level of variety in the output, leading to higher quality and more engaging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4172e566-8859-4f45-a8f5-cc56beecc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566dc9fc-6d75-494e-b89a-0956e8d24e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.45.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a12d99-a779-4357-9375-78a570015250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e99c6f-b0ac-4ee8-bdaa-f01eccd3587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d984ff-adba-4593-83f0-ae19b043f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text \n",
    "def generate_story(prompt, presence_penalty=0, max_tokens=200):\n",
    "    response = client.chat.completions.create(\n",
    "        model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "        messages   = [\n",
    "            {\"role\": \"assistant\", \"content\": f'Generate your answers in {max_tokens} words.'},\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        max_tokens       = max_tokens,\n",
    "        presence_penalty = presence_penalty \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab40bf5-4816-493c-9e69-8c5d6887d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat and'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "max_tokens = 1000\n",
    "prompt = f'''The cat sat on the mat and'''\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61da3a3b-72da-4d1f-95c7-6f4a99bd4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty = 0\n"
     ]
    }
   ],
   "source": [
    "# Low presence penalty\n",
    "pres_penalty = 0\n",
    "\n",
    "print(f\"Presence Penalty = {pres_penalty}\")\n",
    "#print(generate_story(prompt, presence_penalty=pres_penalty))\n",
    "\n",
    "text = generate_story(prompt, presence_penalty=pres_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9a8604-8285-4c44-9cd1-4b79fa6f6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_cat_variations(text):\n",
    "    # Define a pattern to match 'cat', 'cats', and 'cat's' (case insensitive)\n",
    "    pattern = r'\\b(cat|cats|cat\\'s)\\b'\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Count occurrences\n",
    "    counts = {\n",
    "        'cat': matches.count('cat'),\n",
    "        'cats': matches.count('cats'),\n",
    "        \"cat's\": matches.count(\"cat's\")\n",
    "    }\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f146e4f-5ade-4756-b8e3-5dd8c1d0a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 4, 'cats': 0, \"cat's\": 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cat_variations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62d75186-4d6e-4cec-ac0c-6339c514128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty = 2\n"
     ]
    }
   ],
   "source": [
    "# High presence penalty\n",
    "pres_penalty = 2\n",
    "\n",
    "print(f\"Presence Penalty = {pres_penalty}\")\n",
    "#print(generate_story(prompt, presence_penalty=pres_penalty))\n",
    "\n",
    "text = generate_story(prompt, presence_penalty=pres_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1235bb14-a2dc-4cbe-9687-1c7471edba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 4, 'cats': 0, \"cat's\": 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cat_variations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "809adff6-ecb9-4cef-a42e-78893f9bf32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"purred contentedly as the sun streamed through the window, casting a warm glow on its fur. Its eyes half-closed in blissful relaxation, the cat kneaded the soft mat beneath it with gentle paws. The rhythmic sound of purring filled the room, creating a soothing ambiance that seemed to lull anyone nearby into a state of tranquility.\\n\\nThe cat's tail flicked lazily back and forth, occasionally brushing against the floor as it basked in the peaceful moment. With each breath, the cat's chest rose and fell in a steady rhythm, a picture of pure serenity amidst the chaos of daily life.\\n\\nAs the cat stretched out its limbs and shifted slightly on the mat, a sense of calm washed over the room, bringing a moment of quiet joy to all who were lucky enough to witness the simple beauty of a contented feline at rest.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b4ddd-45df-460a-9630-5d367d1ff288",
   "metadata": {},
   "source": [
    "#### Frequency vs presence penalties\n",
    "\n",
    "| **Aspect**                 | **Frequency Penalty**                                                                 | **Presence Penalty**                                                                  |\n",
    "|----------------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| **Purpose**                | Reduces the likelihood of generating tokens based on how often they have been used.  | Discourages the generation of any tokens that have been used at least once.         |\n",
    "| **Implementation**         | Penalty based on the count of occurrences of a token in the current output.          | Binary check: a constant penalty is applied if a token has been used at least once. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f90e3-c743-4e64-a724-fe0f811dc3ce",
   "metadata": {},
   "source": [
    "#### 1. Token Probability Distribution\n",
    "In a language model, each token (word or symbol) has an associated probability that indicates how likely it is to be the next token given the preceding context. This probability is typically calculated using a softmax function.\n",
    "\n",
    "**Softmax Function**\n",
    "The softmax function converts raw scores (logits) into probabilities:\n",
    "\n",
    "$$\\Large [\n",
    "P(y_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "- $P(y_i) $ is the probability of token ( i ).\n",
    "- $( z_i ) $) is the logit score for token ( i ).\n",
    "- The denominator is the sum of exponentials of all logit scores, ensuring that the probabilities sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae521f-1586-4fd0-ac95-7d5398e4b5b3",
   "metadata": {},
   "source": [
    "#### 2. Applying the Presence Penalty\n",
    "The presence penalty modifies the logit score of tokens that have already appeared in the output to reduce their probability. This can be represented mathematically as:\n",
    "\n",
    "$$ \\Large [\n",
    "z_i' = z_i - P \\cdot \\text{presence\\_penalty}(y_i)\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "- \\( z_i' \\) is the modified logit score for token \\( i \\).\n",
    "- \\( P \\) is a constant that determines the strength of the penalty.\n",
    "- \\( \\text{presence\\_penalty}(y_i) \\) is a function that returns a penalty value (typically 1 or a scaling factor) for token \\( y_i \\) if it has already been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef724ff6-5a6a-4bc6-9d95-7a02a426d969",
   "metadata": {},
   "source": [
    "#### 4. Re-calculating Token Probabilities\n",
    "After applying the presence penalty, the new logits \\( z' \\) are used to calculate the updated probabilities using the softmax function:\n",
    "\n",
    "$$\\Large [\n",
    "P'(y_i) = \\frac{e^{z_i'}}{\\sum_{j} e^{z_j'}}\n",
    "]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6f222-1b62-4636-b520-777abdfed4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
