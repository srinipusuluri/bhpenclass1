{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Hashing with HashingVectorizer\n",
    "---------------------------------\n",
    "\n",
    "The __count & tf_idf__ vectorization scheme is simple but the fact that it holds an __in- memory__ mapping from the string tokens to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large datasets:\n",
    "\n",
    "- the `larger the corpus`, the `larger the vocabulary` will grow and hence the memory use too,\n",
    "\n",
    "- building the word-mapping requires a `full pass over the dataset` hence it is `not` possible to fit text classifiers in a strictly __online__ manner.\n",
    "    - Impossibile to do `online` or `out-of-core` / `streaming` learning: the vocabulary_ needs to be learned from the data: its size cannot be known before making one pass over the full dataset\n",
    "    \n",
    "- `pickling` and `un-pickling` vectorizers with a large vocabulary_ can be very `slow` (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),\n",
    "\n",
    "- it is not easily possible to split the vectorization work into concurrent sub tasks as the vocabulary_ attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers’ performance to the point of making them slower than the sequential variant.\n",
    "\n",
    "> It is possible to overcome those limitations by combining the “hashing trick” (Feature hashing) implemented by the sklearn.feature_extraction.FeatureHasher class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.feature_extraction.text.HashingVectorizer \n",
    " - (input='content',  - encoding='utf-8',  - decode_error='strict',  - strip_accents=None,  - lowercase=True,  \n",
    " - reprocessor=None,  - tokenizer=None,  - stop_words=None,  - token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), \n",
    " - analyzer='word',  - n_features=1048576,  - binary=False,  - norm='l2',  - alternate_sign=True, \n",
    " \n",
    " dtype=<class 'numpy.float64'>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert a collection of text documents to a matrix of token occurrences\n",
    "\n",
    "- It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n",
    "\n",
    "- This text vectorizer implementation uses the __hashing__ trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "\n",
    "- it can be used in a streaming (__partial fit__) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "- There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    "    - there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "\n",
    "    - there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "\n",
    "    - no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "The hash function employed is the signed 32-bit version of __Murmurhash3__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameters\n",
    "\n",
    "1. string : {‘filename’, ‘file’, ‘content’}\n",
    "2. lowercase : boolean, default=True\n",
    "3. preprocessor : callable or None (default)\n",
    "4. tokenizer : callable or None (default)\n",
    "5. stop_words : string {‘english’}, list, or None (default)\n",
    "6. ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "7. `n_features` : integer, default=(2 ** 20)\n",
    "The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions\n",
    "8. binary : boolean, default=False.\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "[[-0.53452248  0.53452248  0.         -0.53452248  0.26726124  0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog jumped.\"] \n",
    "\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=6)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.57735027,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.57735027,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.81649658,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.40824829,  0.        ,  0.40824829,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        , -0.70710678,\n",
       "         0.70710678,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.57735027,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.57735027,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
