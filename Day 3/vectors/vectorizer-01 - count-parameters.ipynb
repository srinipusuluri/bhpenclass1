{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "#### Bag of words model\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda-16-FEB\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 10000)\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"It was the best of times it it  it it\" ,\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom and lots of wisdom\",\n",
    "    \"it was the age of foolishness\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'and', 'best', 'foolishness', 'it', 'lots', 'of', 'the',\n",
       "       'times', 'was', 'wisdom', 'worst'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age' 'and' 'best' 'foolishness' 'it' 'lots' 'of' 'the' 'times' 'was'\n",
      " 'wisdom' 'worst']\n",
      "Vocabulary size: 12\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 age\n",
      "       1 and\n",
      "       2 best\n",
      "       3 foolishness\n",
      "       4 it\n",
      "       5 lots\n",
      "       6 of\n",
      "       7 the\n",
      "       8 times\n",
      "       9 was\n",
      "      10 wisdom\n",
      "      11 worst\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 5, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform new test samples\n",
    "test_texts = [\n",
    "    \"Pollution is very bad for health\" ,\n",
    "    \"Govt not very keen on pollution control measures\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "test_dtm = vect_cv.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 (binary representation)\n",
    "\n",
    "- note the default lowercasing of the tokens\n",
    "- stop words are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love apples. Apples are good for health. An apple a day keeps the doctor away\",\n",
    "    \"Play football. It is very exciting. Football is played every where\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an' 'apple' 'apples' 'are' 'away' 'day' 'doctor' 'every' 'exciting'\n",
      " 'football' 'for' 'good' 'health' 'is' 'it' 'keeps' 'love' 'play' 'played'\n",
      " 'the' 'very' 'where']\n",
      "Vocabulary size: 22\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 an\n",
      "       1 apple\n",
      "       2 apples\n",
      "       3 are\n",
      "       4 away\n",
      "       5 day\n",
      "       6 doctor\n",
      "       7 every\n",
      "       8 exciting\n",
      "       9 football\n",
      "      10 for\n",
      "      11 good\n",
      "      12 health\n",
      "      13 is\n",
      "      14 it\n",
      "      15 keeps\n",
      "      16 love\n",
      "      17 play\n",
      "      18 played\n",
      "      19 the\n",
      "      20 very\n",
      "      21 where\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3 - (max_df and min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_list = ['3-idiots', 'Joker', 'Petta', 'Kaappaan', 'Kabir', 'Drishtikone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kabir', '3-idiots', 'Joker', 'Petta']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(movie_list, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kabir', 'Kaappaan', 'Drishtikone', 'Joker']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_names_arrray = random.choices(movie_list, k=4)\n",
    "movie_names_arrray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kabir Kaappaan Drishtikone Joker'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(movie_names_arrray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Petta Kaappaan 3-idiots 3-idiots',\n",
       " 'Drishtikone Kaappaan Drishtikone Joker',\n",
       " 'Kaappaan Joker Kabir Petta',\n",
       " '3-idiots Joker Kaappaan 3-idiots',\n",
       " '3-idiots Drishtikone Kaappaan Kabir',\n",
       " 'Joker Kaappaan Kabir Kabir',\n",
       " '3-idiots Kabir Kaappaan Kabir',\n",
       " 'Kabir Joker 3-idiots Drishtikone',\n",
       " 'Joker Kabir 3-idiots 3-idiots',\n",
       " '3-idiots Drishtikone Drishtikone Drishtikone']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = []\n",
    "np.random.seed(100)\n",
    "\n",
    "for i in range(10):\n",
    "    movie_names_arrray = random.choices(movie_list, k=4)\n",
    "    movie_names_str    = ' '.join(movie_names_arrray)\n",
    "    \n",
    "    movies.append(movie_names_str)\n",
    "    \n",
    "#movies = np.array(movies)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone' 'idiots' 'joker' 'kaappaan' 'kabir' 'petta']\n",
      "Vocabulary size: 6\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 idiots\n",
      "       2 joker\n",
      "       3 kaappaan\n",
      "       4 kabir\n",
      "       5 petta\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 1, 0, 1],\n",
       "       [2, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1],\n",
       "       [0, 2, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 2, 0],\n",
       "       [0, 1, 0, 1, 2, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 2, 1, 0, 1, 0],\n",
       "       [3, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_docs = X_train_cv_dtm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petta           count =   2, DF =   20.00\n",
      "kaappaan        count =   7, DF =   70.00\n",
      "idiots          count =   7, DF =   70.00\n",
      "drishtikone     count =   4, DF =   40.00\n",
      "joker           count =   6, DF =   60.00\n",
      "kabir           count =   6, DF =   60.00\n"
     ]
    }
   ],
   "source": [
    "# count how many times a token appears in the corpus\n",
    "for token in vect_cv.vocabulary_.keys():\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # read each document\n",
    "    for doc in movies:\n",
    "\n",
    "        # check if the token appears in the document, if YES, increment the counter\n",
    "        if re.search(token, str(doc), re.IGNORECASE):\n",
    "            counter +=1\n",
    "    \n",
    "    print('{:15s} count = {:3d}, DF = {:7.2f}'.format(token, counter, (counter/number_docs)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone' 'joker' 'kabir' 'petta']\n",
      "Vocabulary size: 4\n",
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 joker\n",
      "       2 kabir\n",
      "       3 petta\n"
     ]
    }
   ],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(max_df=.65)\n",
    "\n",
    "# train (Bow) \n",
    "vect_cv.fit(movies)\n",
    "\n",
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [2, 1, 0, 0],\n",
       "       [0, 1, 1, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [0, 1, 2, 0],\n",
       "       [0, 0, 2, 0],\n",
       "       [1, 1, 1, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [3, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(movies)\n",
    "\n",
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone' 'idiots' 'joker' 'kaappaan' 'kabir']\n",
      "Vocabulary size: 5\n",
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 idiots\n",
      "       2 joker\n",
      "       3 kaappaan\n",
      "       4 kabir\n"
     ]
    }
   ],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(max_df=.75, min_df=.30)\n",
    "\n",
    "# train (Bow) \n",
    "vect_cv.fit(movies)\n",
    "\n",
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3 - ngram_range or n-gram\n",
    "\n",
    "#### What is an n-gram?\n",
    "\n",
    "An n-gram is a `contiguous sequence` of n __items__ from a given sequence of text. \n",
    "\n",
    "Given a sentence, s, we can construct a list of n-grams from s by finding pairs of words that occur next to each other. \n",
    "\n",
    "Here an __item__ can be a character, a word or a sentence and N can be any integer. \n",
    "\n",
    "- When N is 2, we call the sequence a bigram.\n",
    "- Similarly, a sequence of 3 items is called a trigram, and so on.\n",
    "\n",
    "For example, given the sentence “I am Rajat” you can construct bigrams (n-grams of length 2) by finding consecutive pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"I studied DS/ML/DL at IISc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'studied', 'DS/ML/DL', 'at', 'IISc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = s.split(\" \")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'studied'), ('studied', 'DS/ML/DL'), ('DS/ML/DL', 'at'), ('at', 'IISc')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [(tokens[i],tokens[i+1]) for i in range(0, len(tokens)-1)]\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'studied', 'DS/ML/DL'),\n",
       " ('studied', 'DS/ML/DL', 'at'),\n",
       " ('DS/ML/DL', 'at', 'IISc')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [(tokens[i],tokens[i+1],tokens[i+2]) for i in range(0, len(tokens)-2)]\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"Natural-language processing (NLP) is an area of computer science \" \\\n",
    "    \"and artificial intelligence concerned with the interactions \" \\\n",
    "    \"between computers and human (natural) languages. !!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. !!!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing  nlp  is an area of computer science and artificial intelligence concerned with the interactions between computers and human  natural  languages     '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'is',\n",
       " 'an',\n",
       " 'area',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'natural',\n",
       " 'languages']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'nlp'),\n",
       " ('nlp', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'area'),\n",
       " ('area', 'of'),\n",
       " ('of', 'computer'),\n",
       " ('computer', 'science'),\n",
       " ('science', 'and'),\n",
       " ('and', 'artificial'),\n",
       " ('artificial', 'intelligence'),\n",
       " ('intelligence', 'concerned'),\n",
       " ('concerned', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'interactions'),\n",
       " ('interactions', 'between'),\n",
       " ('between', 'computers'),\n",
       " ('computers', 'and'),\n",
       " ('and', 'human'),\n",
       " ('human', 'natural'),\n",
       " ('natural', 'languages')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = list(ngrams(tokens, 2))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams in vectors for supervised learning problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Penny bought bright blue fishes. !! ) %$&#&#**#*\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish fish .\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\",\n",
    "    \"lets take this sentence for example\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# N-grams (sets of consecutive words) N=2\n",
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 78\n",
      "['and' 'and orange' 'and the' 'at' 'at the' 'ate' 'ate bug' 'ate fish'\n",
      " 'blue' 'blue and' 'blue fishes' 'bought' 'bought bright' 'bright'\n",
      " 'bright blue' 'bug' 'bug and' 'bug it' 'bug penny' 'cat' 'cat ate'\n",
      " 'cat is' 'example' 'fish' 'fish at' 'fish fish' 'fish store' 'fishes'\n",
      " 'for' 'for example' 'is' 'is at' 'is fish' 'is meowing' 'is orange'\n",
      " 'is still' 'it' 'it is' 'it meowed' 'lets' 'lets take' 'meowed'\n",
      " 'meowed once' 'meowing' 'meowing at' 'once' 'once at' 'orange'\n",
      " 'orange fish' 'orange the' 'penny' 'penny ate' 'penny bought' 'penny is'\n",
      " 'penny saw' 'penny went' 'saw' 'saw fish' 'sentence' 'sentence for'\n",
      " 'still' 'still meowing' 'store' 'store penny' 'store the' 'take'\n",
      " 'take this' 'the' 'the bug' 'the cat' 'the fish' 'the store' 'this'\n",
      " 'this sentence' 'to' 'to the' 'went' 'went to']\n",
      "Vocabulary content:\n",
      " {'penny': 50, 'bought': 11, 'bright': 13, 'blue': 8, 'fishes': 27, 'penny bought': 52, 'bought bright': 12, 'bright blue': 14, 'blue fishes': 10, 'and': 0, 'orange': 47, 'fish': 23, 'blue and': 9, 'and orange': 1, 'orange fish': 48, 'the': 67, 'cat': 19, 'ate': 5, 'at': 3, 'store': 62, 'the cat': 69, 'cat ate': 20, 'ate fish': 7, 'fish at': 24, 'at the': 4, 'the store': 71, 'went': 76, 'to': 74, 'bug': 15, 'saw': 56, 'penny went': 55, 'went to': 77, 'to the': 75, 'store penny': 63, 'penny ate': 51, 'ate bug': 6, 'bug penny': 18, 'penny saw': 54, 'saw fish': 57, 'fish fish': 25, 'it': 36, 'meowed': 41, 'once': 45, 'is': 30, 'still': 60, 'meowing': 43, 'it meowed': 38, 'meowed once': 42, 'once at': 46, 'the bug': 68, 'bug it': 17, 'it is': 37, 'is still': 35, 'still meowing': 61, 'meowing at': 44, 'bug and': 16, 'and the': 2, 'the fish': 70, 'cat is': 21, 'is at': 31, 'fish store': 26, 'store the': 64, 'is orange': 34, 'orange the': 49, 'is meowing': 33, 'penny is': 53, 'is fish': 32, 'lets': 39, 'take': 65, 'this': 72, 'sentence': 58, 'for': 28, 'example': 22, 'lets take': 40, 'take this': 66, 'this sentence': 73, 'sentence for': 59, 'for example': 29}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example - 4 (Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(stop_words='english', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "['ate' 'blue' 'bought' 'bright' 'bug' 'cat' 'example' 'fish' 'fishes'\n",
      " 'lets' 'meowed' 'meowing' 'orange' 'penny' 'saw' 'sentence' 'store'\n",
      " 'went']\n",
      "Vocabulary content:\n",
      " {'penny': 13, 'bought': 2, 'bright': 3, 'blue': 1, 'fishes': 8, 'orange': 12, 'fish': 7, 'cat': 5, 'ate': 0, 'store': 16, 'went': 17, 'bug': 4, 'saw': 14, 'meowed': 10, 'meowing': 11, 'lets': 9, 'sentence': 15, 'example': 6}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# notice the lack of stemming .. fish and fishes, meowed\tmeowing\n",
    "\n",
    "# CountVectorizer can \n",
    "# - lowercase letters, \n",
    "# - disregard punctuation and \n",
    "# - stopwords, \n",
    "\n",
    "# but it can't LEMMATIZE or STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n",
      "fish\n",
      "meow\n",
      "meow\n"
     ]
    }
   ],
   "source": [
    "# create the stemmer object\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem(\"fish\"))\n",
    "print(porter_stemmer.stem(\"fishes\"))\n",
    "print(porter_stemmer.stem(\"meowed\"))\n",
    "print(porter_stemmer.stem(\"meowing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use NLTK's PorterStemmer\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re    # regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_string = 'NLPnlp884848^$^$^$&$$&& : -'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLPnlp884848-PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT--PUNCT-'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[^A-Za-z0-9]\", \"-PUNCT-\", text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(ngram_range=(1, 1), stop_words='english', tokenizer=stemming_tokenizer, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "D:\\ANACONDA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                tokenizer=&lt;function stemming_tokenizer at 0x000002012D6CA3E0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;,\n",
       "                tokenizer=&lt;function stemming_tokenizer at 0x000002012D6CA3E0&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(stop_words='english',\n",
       "                tokenizer=<function stemming_tokenizer at 0x000002012D6CA3E0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "['ate' 'blue' 'bought' 'bright' 'bug' 'cat' 'exampl' 'fish' 'let' 'meow'\n",
      " 'onc' 'orang' 'penni' 'saw' 'sentenc' 'store' 'thi' 'went']\n",
      "Vocabulary content:\n",
      " {'penni': 12, 'bought': 2, 'bright': 3, 'blue': 1, 'fish': 7, 'orang': 11, 'cat': 5, 'ate': 0, 'store': 15, 'went': 17, 'bug': 4, 'saw': 13, 'meow': 9, 'onc': 10, 'let': 8, 'thi': 16, 'sentenc': 14, 'exampl': 6}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
